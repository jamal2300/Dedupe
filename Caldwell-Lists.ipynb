{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from future.builtins import next\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import collections\n",
    "import logging\n",
    "import optparse\n",
    "from numpy import nan\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dedupe\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "decrement = True\n",
    "\n",
    "while decrement:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    decrement = False\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        decrement = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Cadwell_List_2.txt\",sep='\\t',header=None,dtype=str,names=['First_Name','Middle_Name','Last_Name','Address1','Address2','City','State','Zip','Lat','Long','Phonenumber','Sex','Age'],low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "da_f=df.loc[:,[\n",
    "            'First_Name',\n",
    "            'Middle_Name',\n",
    "            'Last_Name','Address1','Address2','City','Zip','Phonenumber']]\n",
    "#da_f.head()\n",
    "da_f[da_f['Zip']=='84128'].to_csv(\"Caldwell_List_3.csv\",header=True,index=True,index_label='id',encoding='utf8')\n",
    "da_f.to_csv(\"Caldwell_List_2.csv\",header=True,index=True,index_label='id',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm=pd.read_csv('Data_Model',dtype=str)\n",
    "dm_r=dm.rename(columns={'statevoterid':'id',\n",
    "'vf_firstname':'First_Name',\n",
    "'MiddleInitial':'Middle_Name',\n",
    "'vf_lastname':'Last_Name',\n",
    "'RegistrationAddress1':'Address1',\n",
    "'RegistrationAddress2':'Address2',\n",
    "'RegistrationAddressCity':'City',\n",
    "'RegistrationAddressZip5':'Zip',\n",
    "'landline':'Phonenumber'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_f=dm_r.loc[:,['id',\n",
    "            'First_Name',\n",
    "            'Middle_Name',\n",
    "            'Last_Name','Address1','Address2','City','Zip','Phonenumber']]\n",
    "dm_f.to_csv(\"Data_Model_dedupe.csv\",header=True,index=False,encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#da=pd.read_csv(\"Caldwell_List.csv\")\n",
    "dm=pd.read_csv('Data_Model_dedupe.csv',dtype=str)\n",
    "dm[dm['Zip']=='84128'].to_csv('Data_Model_dedupe_1.csv',header=True,index=False,encoding='utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm1=pd.read_csv('Data_Model_dedupe_1.csv',dtype=str)\n",
    "dm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filet = 'Caldwell_List_3.csv'\n",
    "input_filet1= 'Data_Model_dedupe_1.csv'\n",
    "input_file = 'Caldwell_List_2.csv'\n",
    "input_file1= 'Data_Model_dedupe.csv'\n",
    "output_file = 'data_matching_output.csv'\n",
    "settings_file = 'data_matching_learned_settings'\n",
    "training_file = 'data_matching_training.json'\n",
    "\n",
    "\n",
    "def preProcess(column):\n",
    "#\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column :\n",
    "        column = None\n",
    "    return column\n",
    "\n",
    "def readData(filename):\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID and each value is dict\n",
    "    \"\"\"\n",
    "\n",
    "    data_d = {}\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in tqdm(reader):\n",
    "            clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
    "            row_id = int(row['id'])\n",
    "            data_d[row_id] = dict(clean_row)\n",
    "\n",
    "    return data_d\n",
    "\n",
    "\n",
    "print('importing data ...')\n",
    "data_1 = readData(input_file)\n",
    "data_2 = readData(input_file1)\n",
    "data_3 = readData(input_filet)\n",
    "data_4 = readData(input_filet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Training\n",
    "\n",
    "if os.path.exists(settings_file):\n",
    "    print('reading from', settings_file)\n",
    "    with open(settings_file, 'rb') as f:\n",
    "        deduper = dedupe.StaticDedupe(f)\n",
    "\n",
    "else:\n",
    "    # Define the fields dedupe will pay attention to\n",
    "    #\n",
    "    # Notice how we are telling dedupe to use a custom field comparator\n",
    "    # for the 'Zip' field.\n",
    "    fields = [\n",
    "        {'field' : 'First_Name', 'type': 'String'},\n",
    "        {'field' : 'Middle_Name', 'type': 'String'},\n",
    "        {'field' : 'Last_Name', 'type': 'String'},\n",
    "        {'field' : 'Address1', 'type': 'String'},\n",
    "        {'field' : 'Address2', 'type': 'String','has missing' : True},\n",
    "        {'field' : 'City', 'type': 'String'},\n",
    "        {'field' : 'Zip', 'type': 'Exact', 'has missing' : False},\n",
    "        {'field' : 'Phonenumber', 'type': 'String', 'has missing' : True},\n",
    "        ]\n",
    "\n",
    "    # Create a new deduper object and pass our data model to it.\n",
    "    linker = dedupe.RecordLink(fields)\n",
    "    linker.sample(data_1, data_2, 15000)\n",
    "    \n",
    "        # If we have training data saved from a previous run of linker,\n",
    "    # look for it an load it in.\n",
    "    # __Note:__ if you want to train from scratch, delete the training_file\n",
    "    if os.path.exists(training_file):\n",
    "        print('reading labeled examples from ', training_file)\n",
    "        with open(training_file) as tf :\n",
    "            linker.readTraining(tf)\n",
    "\n",
    "    # ## Active learning\n",
    "    # Dedupe will find the next pair of records\n",
    "    # it is least certain about and ask you to label them as matches\n",
    "    # or not.\n",
    "    # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "    # press 'f' when you are finished\n",
    "    print('starting active labeling...')\n",
    "\n",
    "    dedupe.consoleLabel(linker)\n",
    "\n",
    "    linker.train()\n",
    "\n",
    "    # When finished, save our training away to disk\n",
    "    with open(training_file, 'w') as tf :\n",
    "        linker.writeTraining(tf)\n",
    "\n",
    "    # Save our weights and predicates to disk.  If the settings file\n",
    "    # exists, we will skip all the training and learning next time we run\n",
    "    # this file.\n",
    "    with open(settings_file, 'wb') as sf :\n",
    "        linker.writeSettings(sf)\n",
    "\n",
    "\n",
    "# ## Blocking\n",
    "\n",
    "# ## Clustering\n",
    "\n",
    "# Find the threshold that will maximize a weighted average of our\n",
    "# precision and recall.  When we set the recall weight to 2, we are\n",
    "# saying we care twice as much about recall as we do precision.\n",
    "#\n",
    "# If we had more data, we would not pass in all the blocked data into\n",
    "# this function but a representative sample.\n",
    "\n",
    "print('clustering...')\n",
    "linked_records = linker.match(data_1, data_2, 0)\n",
    "\n",
    "print('# duplicate sets', len(linked_records))\n",
    "\n",
    "# ## Writing Results\n",
    "\n",
    "# Write our original data back out to a CSV with a new column called \n",
    "# 'Cluster ID' which indicates which records refer to each other.\n",
    "\n",
    "cluster_membership = {}\n",
    "cluster_id = None\n",
    "for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "    for record_id in cluster:\n",
    "        cluster_membership[record_id] = (cluster_id, score)\n",
    "\n",
    "if cluster_id :\n",
    "    unique_id = cluster_id + 1\n",
    "else :\n",
    "    unique_id =0\n",
    "    \n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header_unwritten = True\n",
    "\n",
    "    for fileno, filename in enumerate(('Caldwell_List_2.csv', 'Data_Model_dedupe.csv')) :\n",
    "        with open(filename) as f_input :\n",
    "            reader = csv.reader(f_input)\n",
    "\n",
    "            if header_unwritten :\n",
    "                heading_row = next(reader)\n",
    "                heading_row.insert(0, 'source file')\n",
    "                heading_row.insert(0, 'Link Score')\n",
    "                heading_row.insert(0, 'Cluster ID')\n",
    "                writer.writerow(heading_row)\n",
    "                header_unwritten = False\n",
    "            else :\n",
    "                next(reader)\n",
    "\n",
    "            for row_id, row in enumerate(reader):\n",
    "                cluster_details = cluster_membership.get(filename + str(row_id))\n",
    "                if cluster_details is None :\n",
    "                    cluster_id = unique_id\n",
    "                    unique_id += 1\n",
    "                    score = None\n",
    "                else :\n",
    "                    cluster_id, score = cluster_details\n",
    "                row.insert(0, fileno)\n",
    "                row.insert(0, score)\n",
    "                row.insert(0, cluster_id)\n",
    "                writer.writerow(row)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do=pd.read_csv('data_matching_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_dm=do[do['source file']==1]\n",
    "matched_dm.rename(columns={'Cluster ID':'cluster_id',\n",
    "'Link Score':'Link_Score',\n",
    "'source file':'source_file'}).to_csv(\"matched_data_model.csv\",header=True,index=False,encoding='utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_dm=pd.read_csv('matched_data_model.csv')\n",
    "mat_dm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_cl=do[do['source file']==0]\n",
    "matched_cl.rename(columns={'Cluster ID':'cluster_id',\n",
    "'Link Score':'Link_Score',\n",
    "'source file':'source_file'}).to_csv(\"matched_caldwell.csv\",header=True,index=False,encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_cl=pd.read_csv('matched_caldwell.csv')\n",
    "mat_cl.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
